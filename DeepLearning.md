# Deep Learning

# Table of Contents
- [Neural Networks](#data-structures)
  - [architecture](#Architecture)
  - [activation functions](#linked-list)
  - [forward propagation](#forward-propagation)
  - [loss function](#loss-function)
  - [gradient descent](#gradient-descent)
  - [back propagation](#back-propagation)

- [Improving Neural Networks](#improving-neural-networks)
  - [Train/Dev/Test Sets](#train-dev-test-sets)
  - [Bias/Variance](#bias-variance)
  - [Basic Recipe for Machine Learning](#basic-recipe-for-machine-learning)
  - [Regularization](#regularization)
    - [Why Regularization Reduces Overfitting?](#why-regularization-reduces-overfitting)
    - [Dropout Regularization](#dropout-regularization)
    - [Understanding Dropout](#understanding-dropout)
    - [Other Regularization Methods](#other-regularization-methods)
  - [Normalizing Inputs](#normalizing-inputs)
  - [Vanishing/Exploding Gradients](#vanishing-exploding-gradients)
  - [Weight Initialization in Deep Network](#weight-initialization-in-deep-network)
  - [Numerical Approximation of Gradients](#numerical-approximation-of-gradients)
    - [Gradient Checking](#gradient-checking)
    - [Mini Batch Gradient Descent](#mini-batch-gradient-descent)
    - [Exponentially Weighted Averages](#exponentially-weighted-averages)
    - [Gradient Descent with Momentum](#gradient-descent-with-momentum)
    - [RMS Prop](#rms-prop)
    - [Adam Optimization Algorithm](#adam-optimization-algorithm)
    - [Learning Rate Decay](#learning-rate-decay)
  - [Tuning Process](#tuning-process)
  - [Using an Appropriate Scale](#using-an-appropriate-scale)

  
